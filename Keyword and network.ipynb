{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Vietnamese stopword here: https://github.com/stopwords/vietnamese-stopwords\n",
    "# Dowload Vncorenlp here: https://github.com/vncorenlp/VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from vncorenlp import VnCoreNLP\n",
    "!pip install networkx\n",
    "!pip install googletrans\n",
    "from googletrans import Translator\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# !pip install vncorenlp\n",
    "data = pd.read_excel('insert content data here.xlsx',index_col=None,na_values=['NA'],usecols=\"B\")\n",
    "sample_data = data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "stop_words = [line.strip() for line in open('vietnamese-stopwords.txt','r', encoding =\"utf8\")]\n",
    "stop_words = set([w.lower().replace(' ','_') for w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = VnCoreNLP(\"vncore_nlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos\", max_heap_size='-Xmx4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEB_ADDRESS_RE = r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_sentences(content):\n",
    "    content = re.sub(WEB_ADDRESS_RE, ' ', content)\n",
    "    content = content.strip().split('\\n')\n",
    "    content = [string for string in content if string != \"\"]\n",
    "    clean_sentences = []\n",
    "    for p in content:\n",
    "        sentences = annotator.annotate(p)['sentences']\n",
    "        for s in sentences:\n",
    "            words = [word['form'].lower() for word in s if word['posTag'].startswith('N')]\n",
    "            words = [w for w in words if w not in string.punctuation and w not in stop_words]\n",
    "            clean_sentences.append(words)\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "start=datetime.now()\n",
    "clean_sentences=[]\n",
    "for _, row in sample_data.iterrows():\n",
    "    clean_sentence = get_clean_sentences(row['content'])\n",
    "    clean_sentences.append(clean_sentence)\n",
    "clean_sentences\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list\n",
    "start=datetime.now()\n",
    "words =[]\n",
    "for sentences in clean_sentences:\n",
    "    for sentence in sentences:\n",
    "        words.append(sentence)\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define most common word\n",
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "common=[]\n",
    "for sentence in words:\n",
    "    for word in sentence:\n",
    "        common.append(word)\n",
    "most_common = Counter(common).most_common(50)\n",
    "print(datetime.now()-start)\n",
    "# set(list(dict(most_common).keys()))\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in most_common:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count #co-occurences\n",
    "co_occurence_graph = {}\n",
    "start=datetime.now()\n",
    "for word in words:\n",
    "    token_pairs = combinations(word,2)\n",
    "    for u, v in token_pairs:\n",
    "        if u in set(list(dict(most_common).keys())) and v in set(list(dict(most_common).keys())):\n",
    "            if u != v:\n",
    "                if u in co_occurence_graph:\n",
    "                    if v in co_occurence_graph[u]:\n",
    "                        co_occurence_graph[u][v]['weight'] += 1\n",
    "                    else:\n",
    "                        co_occurence_graph[u][v] = {'weight':1}\n",
    "                else:\n",
    "                    co_occurence_graph[u] = {v:{'weight':1}}\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to networkx graph                    \n",
    "co_occurence_graph = nx.from_dict_of_dicts(co_occurence_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run k-core procedure\n",
    "g = nx.k_core(co_occurence_graph,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to gephi input\n",
    "nx.write_gexf(co_occurence_graph, \"period_all.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export nodes\n",
    "nodes=[]\n",
    "for node in co_occurence_graph.nodes:\n",
    "    nodes.append(node)\n",
    "nodes_all = open(\"nodes.txt\", \"w\",encoding=\"utf8\")\n",
    "for element in nodes:\n",
    "    nodes_all.write(element+'\\n')\n",
    "nodes_all.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export edges\n",
    "nx.write_edgelist(co_occurence_graph, \"edges_all.txt\", delimiter=' ', data=['weight'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
